<!doctype html><html><head><title>The Limited Power of AI: Adversarial Attacks & Defenses!</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/favicon_hu919edbd16c020569417749c51addca8d_630971_42x0_resize_box_2.png><link rel=stylesheet href=/css/style.css><meta name=description content="The Limited Power of AI: Adversarial Attacks & Defenses!"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/main-logo_hu919edbd16c020569417749c51addca8d_630971_42x0_resize_box_2.png alt=Logo>Ghada</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/main-logo_hu919edbd16c020569417749c51addca8d_630971_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/images/inverted-logo_hu919edbd16c020569417749c51addca8d_630971_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=https://GhadaJouini.github.io/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://GhadaJouini.github.io/posts/blog2/hero.svg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/ph1.jpg alt="Author Image"><h5 class=author-name>Ghada Jouini</h5><p>March 18, 2021</p></div><div class=title><h1>The Limited Power of AI: Adversarial Attacks & Defenses!</h1></div><div class=post-content id=post-content><p><img src=https://cdn-images-1.medium.com/max/2000/1*-Vy_RsbD2XU9SDbVSRwHGA.jpeg alt></p><h2 id=introduction><strong>Introduction</strong></h2><blockquote><p><strong>Is ML truly ready for real-world deployment?</strong></p></blockquote><p>Over the last years, state-of-the-art Deep Learning models are highly effective in solving many complex real-world problems — boosting progress in computer vision and natural language processing and impacting nearly every industry. However, these models are vulnerable to well-designed input samples called adversarial examples.</p><p>With the advance in AI systems, most companies are focusing on increasing AI performance rather than quality and safety. To this day researchers are exploring new approaches to address <strong>Robustness, Safety, and Security</strong> in AI.</p><p>Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. In this article, we are going to understand what is an adversarial example and what are the most common adversarial defenses.</p><p><img src=https://cdn-images-1.medium.com/max/2000/1*P6D47Tqo-eHzNUgTS0nsUA.gif alt="Synthesizing Robust Adversarial Examples: Adversarial Turtle"></p><h2 id=adversarial-attacks-history>Adversarial Attacks History</h2><p>Adversarial examples are not a new phenomenon. The “newness” of adversarial examples came from them being discovered in the context of deep learning. In 2013 Christian Swegedy from Google AI was trying to figure out how neural nets “think” but instead he ended up at discovering new phenomena, the “<a href=https://arxiv.org/abs/1312.6199>Intriguing property</a>” meaning that any machine learning classifier can be tricked to give incorrect predictions, and with a little bit of skill, you can get them to give pretty much any result you want.</p><p><img src=https://cdn-images-1.medium.com/max/2000/1*uUH2AHIsCdqQAEubnmvXMQ.png alt></p><h2 id=what-is-adversarial-attacks>What is Adversarial attacks?</h2><p>Machine learning algorithms accept inputs as numeric vectors. Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake.</p><p><strong>How is this possible?</strong> Machine Learning models consist of a series of specific transformations, and most of these transformations turn out to be very sensitive to slight changes in input. Harnessing this sensitivity and exploiting it to modify an algorithm’s behavior is an important problem in AI security.</p><p>To get a better idea of what adversarial examples look like, let’s consider this demonstration from <a href=https://arxiv.org/abs/1412.6572>Explaining and Harnessing Adversarial examples</a>: starting with an input image of a panda, the attacker adds a meticulous perturbation that has been calculated to make the image be recognized as a gibbon with high confidence.</p><p><img src=https://cdn-images-1.medium.com/max/3196/1*ZHadZlavwmVLnKpHoP8_IQ.png alt="An adversarial input, overlaid on a typical image, can cause a classifier to miscategorize a panda as a gibbon."></p><h2 id=physical-adversarial-attacks>Physical Adversarial Attacks</h2><p>Adversarial examples have the potential to be dangerous. For example, attackers could target autonomous vehicles by using stickers or paint to create an adversarial stop sign that the vehicle would interpret as a ‘yield’ or other sign, this could be a huge problem of safety in autonomous driving cars.</p><p><img src=https://cdn-images-1.medium.com/max/2000/1*jEWbNTZcetqS0Sf5QIjYpA.png alt></p><h2 id=types-of-adversarial-attacks>Types of Adversarial Attacks</h2><p>Here are the main types of hacks we will focus on:</p><ol><li><p><strong>Non-targeted adversarial attack:</strong> the most general type of attack when all you want to do is to make the classifier give an incorrect result.</p></li><li><p><strong>Targeted adversarial attack:</strong> a slightly more difficult attack that aims to receive a particular class for your input.</p></li></ol><h2 id=why-do-adversarial-examples-exist>Why do Adversarial Examples Exist?</h2><p>The first hypothesis was introduced by <a href=https://arxiv.org/abs/1312.6199>Szegedy’s paper</a>, where he explained that the existence of adversarial attacks is due to the presence of low probability “pockets” in the manifold (ie <strong>too much non-linearity</strong>) and poor regularization of networks.</p><p>Later On, an opposing theory, pioneered by <a href=https://arxiv.org/abs/1412.6572>Goodfellow</a>, arguing that in fact, adversarial examples occurred due to **too much linearity **in modern machine learning and especially deep learning systems.</p><p>Goodfellow argued that activation functions like ReLU and Sigmoid are straight lines in the middle (where it just so happens we prefer to keep our gradients to prevent them from exploding or vanishing). And so really inside a neural net, you have a ton of linear functions, all perpetuating one another’s input, all in the same direction. If you then add tiny perturbations to some of the inputs (a few pixels here and there) that accumulate into massive difference on the other end of the network and it spits out gibberish.</p><p>The third adopted hypothesis is the <a href=https://arxiv.org/abs/1608.07690>tilted boundary</a>. In a nutshell, the authors argue that because the model never fits the data perfectly (otherwise test set accuracy would have always been 100%) — there will always be adversarial pockets of inputs that exist between the boundary of the classifier and the actual sub-manifold of sampled data. They also empirically debunk the previous two approaches.</p><p>Finally, <a href=https://arxiv.org/abs/1905.02175>A recent paper from MIT</a> argues that adversarial examples are not a bug — <strong>they’re a feature of how neural networks see the world</strong>. Just because we humans are limited to 3 dimensions and can’t distinguish noise patterns from one another doesn’t mean those noise patterns are not good features. Our eyes are just bad sensors. So what we’re dealing with here is a pattern recognition machine more sophisticated than ourselves.</p><p><img src=https://cdn-images-1.medium.com/max/2000/1*vUeeA31IIgnvXPxPcgE3iA.png alt></p><h2 id=adversarial-examples-transferability><strong>Adversarial Examples Transferability</strong></h2><p>Transferability is an important phenomenon in adversarial examples. previous work shows that adversarial examples transfer between different model architectures and different datasets.</p><h2 id=defenses-against-adversarial-attacks>Defenses Against Adversarial Attacks</h2><p>In this section, we discuss current state-of-the-art defenses against adversarial attacks and their limitations. These represent recent defense approaches, each of which was quite effective until being adaptively attacked.</p><p>Existing defenses either make it more difficult to compute adversarial examples or try to detect them at inference time using properties of the model.</p><p><em><strong>Existing Defenses:</strong></em></p><ol><li><p><strong>Adversarial Training:</strong> The primary objective of adversarial training is to increase model robustness by injecting adversarial examples into the training set. However, adversarial training only defends the model against the same attacks used to craft the examples originally included in the training pool. <strong>But what if we have a new attack?</strong> the adversarial will definitely fool the model as if no defense was in place. We could keep retraining the model including newly forged adversarial examples over and over but at some point, we’ve inserted so much fake data into the training set that the boundary the model learns becomes useless.</p></li><li><p><strong>Gradient Masking:</strong> The defender trains a model with small gradients. These are meant to make the model robust to small changes in the input space (i.e. adversarial perturbations). The reason Gradient masking doesn’t work as a defense is because of adversarial examples <strong>transferability</strong> property, even if we succeed at hiding the gradients of the model — the attacker can still build a surrogate, attack it, then transfer the examples.</p></li></ol><p><em><strong>Existing Detection Methods:</strong></em></p><ol><li><p><strong>Feature Squeezing:</strong> The main idea behind this defense is that it reduces the complexity of representing the data so that the adversarial perturbations disappear because of low sensitivity. There are mainly two heuristics behind the approach: <em><strong>Reduce the color depth on a pixel level</strong></em> and <em><strong>Use of a smoothing filter over the images</strong></em>. Despite the fact that these techniques work well in preventing some attacks, Feature squeezing performs poorly against others (FGSM, BIM).</p></li><li><p><strong>MagNet:</strong> It takes a two-pronged approach: it has a detector that flags adversarial examples and a reformer that transforms adversarial examples into benign ones. However, MagNet is vulnerable to adaptive adversarial attacks.</p></li><li><p><strong>Local Intrinsic Dimensionality (LID)</strong>: LID measures a model’s internal dimensionality characteristics. LID is vulnerable to high confidence adversarial examples.</p></li></ol><h2 id=-the-discovery-of-new-approach-to-defend-against-adversarial-attack>💡 The discovery of new approach to defend against Adversarial Attack</h2><p>In this section we will discover a new paradigm against adversarial attacks using <a href=https://arxiv.org/abs/1904.08554>Honeypots to Catch Adversarial Aacks on Neural Networks</a></p><p>History suggests it might be difficult, near impossible to prevent adversaries from computing effective adversarial examples. Recently, a group of researchers from SANDLAB, <strong>Shan et al.</strong>(CCS’20) proposed a honeypots-based defense against adversarial examples, called <strong>trapdoor-enabled defense</strong>.</p><h2 id=-key-intuition-of-the-defense-idea>💡 Key Intuition of the defense idea:</h2><p>In this paper Shan et al. take a different approach: instead of focusing on the characteristics of adversarial samples, they consider characteristics of the attacking process. They view an attack as an optimization process with respect to some attack objective functions, and the goal is to proactively modify such objective functions so as to confuse and mislead the attacker.</p><p>The concept was inspired and implemented using backdoor poisoning techniques.</p><h2 id=-paper-contribution>💥 Paper Contribution:</h2><p><img src=https://cdn-images-1.medium.com/max/2000/1*50GHXl_FWgkh8z17qD-cmA.png alt></p><p>The defense injects a backdoor into a neural network during training and then shows that adversarial examples generated on this classifier share similar activation patterns to backdoored inputs — and can therefore be detected through trapdoor with near-perfect accuracy.</p><p>The Honeypot Defense injects a backdoor perturbation ∆ unique to a particular label y_t during the neural network training process so that for all inputs x, the classifier will consistently and predictably misclassify f(x + ∆).</p><blockquote><p><strong>f: X → Y represents a neural network classifier that maps the input space X to the set of classification labels Y</strong></p></blockquote><p>As a result of this backdoor, standard methods to generate adversarial examples will create examples x’ that have similar “<strong>characteristics</strong>” of the backdoored inputs. These characteristics are formalized by comparing the similarity(cosine similarity) between the input and the trapdoor in feature space and therefore produce similar activation patterns. If the similarity is higher than a threshold, the model will flag the input as adversarial.</p><h2 id=-model-architecture>🧠 Model Architecture:</h2><p><img src=https://cdn-images-1.medium.com/max/3004/1*6r-6Q1FnCw3qAHyd7H03cg.jpeg alt="This figure presents a high-level illustration of the defense. From the author paper"></p><ul><li><p>(a) First, we choose to defend either a single label or multiple labels.</p></li><li><p>(b) Second, for each protected label y, we train a distinct trapdoor into the model to defend against adversarial misclassification to y.</p></li><li><p>(c) For each embedded trapdoor, we compute its trapdoor signature, and detect adversarial attacks that exhibit similar activation patterns.</p></li></ul><p><img src=https://cdn-images-1.medium.com/max/2000/1*uGNpOwqIrbkhe8F9ibT-Qg.jpeg alt></p><h2 id=conclusion>Conclusion</h2><p>Building and testing robust machine-learning systems is sure to be a challenge for years to come. As ML and AI become increasingly integral to different facets of society, it is crucial that we bear in mind their limitations and risks, and design processes to ameliorate them.</p><blockquote><p><strong>In few years, much of what we call AI won’t be considered AI without lifelong learning and Robustness !</strong></p></blockquote><h2 id=-happy-reading-everyone-><strong>💡 Happy Reading Everyone !</strong></h2></div><div class=btn-improve-page><a href=https://github.com/GhadaJouini/GhadaJouini.github.io/edit//content/posts/Blog2/index.md.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/search/ title="Search Results" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>Prev</div><div class=next-prev-text>Search Results</div></a></div><div class="col-md-6 next-article"><a href=/posts/blog1/ title="The Far Side of AI: Neuro-Symbolic AI" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>The Far Side of AI: Neuro-Symbolic AI</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#introduction><strong>Introduction</strong></a></li><li><a href=#adversarial-attacks-history>Adversarial Attacks History</a></li><li><a href=#what-is-adversarial-attacks>What is Adversarial attacks?</a></li><li><a href=#physical-adversarial-attacks>Physical Adversarial Attacks</a></li><li><a href=#types-of-adversarial-attacks>Types of Adversarial Attacks</a></li><li><a href=#why-do-adversarial-examples-exist>Why do Adversarial Examples Exist?</a></li><li><a href=#adversarial-examples-transferability><strong>Adversarial Examples Transferability</strong></a></li><li><a href=#defenses-against-adversarial-attacks>Defenses Against Adversarial Attacks</a></li><li><a href=#-the-discovery-of-new-approach-to-defend-against-adversarial-attack>💡 The discovery of new approach to defend against Adversarial Attack</a></li><li><a href=#-key-intuition-of-the-defense-idea>💡 Key Intuition of the defense idea:</a></li><li><a href=#-paper-contribution>💥 Paper Contribution:</a></li><li><a href=#-model-architecture>🧠 Model Architecture:</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#-happy-reading-everyone-><strong>💡 Happy Reading Everyone !</strong></a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=/#achievements>Achievements</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>jouini.ghada1@gmail.com</span></li><li><span>Phone:</span> <span>+216 58 713 454</span></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script></body></html>